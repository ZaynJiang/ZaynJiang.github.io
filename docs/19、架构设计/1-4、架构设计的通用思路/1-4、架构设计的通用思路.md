## 1. 开头  
&emsp;&emsp;前面我们介绍了架构设计的一些概念，以及分析了系统复杂度的来源，并知道了架构设计的宏观上的基本原则。那么当我们真要给某系统做架构设计时，有没有通用的设计思路呢？答案是有的。我们在对系统进行架构设计时，完全可以遵循该思路。主要的思路是：
* 识别系统复杂度
* 设计备选方案
* 评估和选择备选方案
* 详细方案设计  

&emsp;&emsp;主要是按照这几个大的步骤来思考即可。  


## 2. 识别复杂度  
### 2.1. 如何识别复杂度  
&emsp;&emsp;架构设计的本质目的是为了解决软件系统的复杂性，所以在我们设计架构时，首先就要分析系统的复杂性。只有正确分析出了系统的复杂性，后续的架构设计方案才不会偏离方向；否则，如果对系统的复杂性判断错误，即使后续的架构设计方案再完美再先进，都是南辕北辙，做的越好，错的越多、越离谱。  
&emsp;&emsp;例如，如果一个系统的复杂度本来是业务逻辑太复杂，功能耦合严重，架构师却设计了一个 TPS 达到 50000/ 秒的高性能架构，即使这个架构最终的性能再优秀也没有任何意义，因为架构没有解决正确的复杂性问题。  
&emsp;&emsp;架构的复杂度主要来源于“高性能”“高可用”“可扩展”等几个方面，但架构师在具体判断复杂性的时候，不能生搬硬套，认为任何时候架构都必须同时满足这三方面的要求。实际上大部分场景下，复杂度只是其中的某一个，少数情况下包含其中两个，如果真的出现同时需要解决三个或者三个以上的复杂度，要么说明这个系统之前设计的有问题，要么可能就是架构师的判断出现了失误，即使真的认为要同时满足这三方面的要求，也必须要进行优先级排序。   
&emsp;&emsp;例如，专栏前面提到过的“亿级用户平台”失败的案例，设计对标腾讯的 QQ，按照腾讯 QQ 的用户量级和功能复杂度进行设计，高性能、高可用、可扩展、安全等技术一应俱全，一开始就设计出了 40 多个子系统，然后投入大量人力开发了将近 1 年时间才跌跌撞撞地正式上线。上线后发现之前的过度设计完全是多此一举，而且带来很多问题：  
* 系统复杂无比，运维效率低下，每次业务版本升级都需要十几个子系统同步升级，操作步骤复杂，容易出错，出错后回滚还可能带来二次问题。

* 每次版本开发和升级都需要十几个子系统配合，开发效率低下。

* 子系统数量太多，关系复杂，小问题不断，而且出问题后定位困难。

* 开始设计的号称 TPS 50000/ 秒的系统，实际 TPS 连 500 都不到。
  
&emsp;&emsp;由于业务没有发展，最初的设计人员陆续离开，后来接手的团队，无奈又花了 2 年时间将系统重构，合并很多子系统，将原来 40 多个子系统合并成不到 20 个子系统，整个系统才逐步稳定下来。  
&emsp;&emsp;如果运气真的不好，接手了一个每个复杂度都存在问题的系统，那应该怎么办呢？答案是一个个来解决问题，不要幻想一次架构重构解决所有问题。例如这个“亿级用户平台”的案例，后来接手的团队其实面临几个主要的问题：系统稳定性不高，经常出各种莫名的小问题；系统子系统数量太多，系统关系复杂，开发效率低；不支持异地多活，机房级别的故障会导致业务整体不可用。如果同时要解决这些问题，就可能会面临这些困境：   
* 要做的事情太多，反而感觉无从下手。
* 设计方案本身太复杂，落地时间遥遥无期。
* 同一个方案要解决不同的复杂性，有的设计点是互相矛盾的。例如，要提升系统可用性，就需要将数据及时存储到硬盘上，而硬盘刷盘反过来又会影响系统性能  

&emsp;&emsp;因此，正确的做法是将主要的复杂度问题列出来，然后根据业务、技术、团队等综合情况进行排序，优先解决当前面临的最主要的复杂度问题。“亿级用户平台”这个案例，团队就优先选择将子系统的数量降下来，后来发现子系统数量降下来后，不但开发效率提升了，原来经常发生的小问题也基本消失了，于是团队再在这个基础上做了异地多活方案，也取得了非常好的效果。   
&emsp;&emsp;对于按照复杂度优先级解决的方式，存在一个普遍的担忧：如果按照优先级来解决复杂度，可能会出现解决了优先级排在前面的复杂度后，解决后续复杂度的方案需要将已经落地的方案推倒重来。这个担忧理论上是可能的，但现实中几乎是不可能出现的，原因在于软件系统的可塑性和易变性。对于同一个复杂度问题，软件系统的方案可以有多个，总是可以挑出综合来看性价比最高的方案。   
&emsp;&emsp;即使架构师决定要推倒重来，这个新的方案也必须能够同时解决已经被解决的复杂度问题，一般来说能够达到这种理想状态的方案基本都是依靠新技术的引入。例如，Hadoop 能够将高可用、高性能、大容量三个大数据处理的复杂度问题同时解决。   
&emsp;&emsp;识别复杂度对架构师来说是一项挑战，因为原始的需求中并没有哪个地方会明确地说明复杂度在哪里，需要架构师在理解需求的基础上进行分析。有经验的架构师可能一看需求就知道复杂度大概在哪里；如果经验不足，那只能采取“排查法”，从不同的角度逐一进行分析。  

### 2.2. 识别复杂度案例    

#### 2.2.1. 案例背景 
&emsp;&emsp;做一个微博系统，业务发展很快，系统也越来越多，系统间协作的效率很低：  
* 用户发一条微博后，微博子系统需要通知审核子系统进行审核，然后通知统计子系统进行统计，再通知广告子系统进行广告预测，接着通知消息子系统进行消息推送……
* 用户等级达到 VIP 后，等级子系统要通知福利子系统进行奖品发放     

&emsp;&emsp;这些问题背后的根源在于架构上各业务子系统强耦合，而消息队列系统正好可以完成子系统的解耦，于是提议要引入消息队列系统。经过一分析二讨论三开会四汇报五审批等一系列操作后，消息队列系统终于立项了。 其它背景信息：  
* 中间件团队规模不大，大约 6 人左右。
* 中间件团队熟悉 Java 语言，但有一个新同事 C/C++ 很牛。
* 开发平台是 Linux，数据库是 MySQL。
* 目前整个业务系统是单机房部署，没有双机房。   

#### 2.2.2. 分析过程
针对前浪微博的消息队列系统，采用“排查法”来分析复杂度，具体分析过程是：  
* 这个消息队列是否需要高性能  
  当前业务规模计算的性能要求并不高，但业务会增长，因此系统设计需要考虑一定的性能余量。由于现在的基数较低，为了预留一定的系统容量应对后续业务的发展，我们将设计目标设定为峰值的 4 倍，因此最终的性能要求是：TPS 为 1380，QPS 为 13800    
  
**PS：对于架构师来说，常见系统的性能量级需要烂熟于心，例如nginx负载均衡性能是3万左右，mc的读取性能5万左右，kafka号称百万级，zookeeper写入读取2万以上，http请求访问大概在2万左右，具体的数值和机器配置以及测试案例有关，但大概的量级不会变化很大。如果是业务系统，由于业务复杂度差异很大，有的每秒500请求可能就是高性能了，因此需要针对业务进行性能测试，确立性能基线，方便后续架构设计做比较。**

* 消息队列是否需要高可用性  
  微博子系统来说，如果消息丢了，导致没有审核，然后触犯了国家法律法规，则是非常严重的事情；对于等级子系统来说，如果用户达到相应等级后，系统没有给他奖品和专属服务，则 VIP 用户会很不满意，导致用户流失从而损失收入，虽然也比较关键，但没有审核子系统丢消息那么严重  
  综合来看，消息队列需要高可用性，包括消息写入、消息存储、消息读取都需要保证高可用性  

* 这个消息队列是否需要高可扩展性  
  基本无须扩展  

综合分析下来，消息队列的复杂性主要体现在这几个方面：高性能消息读取、高可用消息写入、高可用消息存储、高可用消息读取  


#### 2.3. 小结   
架构设计由需求所驱动，本质目的是为了解决软件系统的复杂性；为此，我们在进行架构设计时，需要以理解需求为前提，首要进行系统复杂性的分析。具体做法是：
* 构建复杂度的来源清单——高性能、可用性、扩展性、安全、低成本、规模等。
* 结合需求、技术、团队、资源等对上述复杂度逐一分析是否需要？是否关键？
  * “高性能”主要从软件系统未来的TPS、响应时间、服务器资源利用率等客观指标，也可以从用户的主观感受方面去考虑。
  * “可用性”主要从服务不中断等质量属性，符合行业政策、国家法规等方面去考虑。
  * “扩展性”则主要从功能需求的未来变更幅度等方面去考虑。
* 按照上述的分析结论，得到复杂度按照优先级的排序清单，越是排在前面的复杂度，就越关键，就越优先解决
* 需要特别注意的是：随着所处的业务阶段不同、外部的技术条件和环境的不同，得到的复杂度问题的优先级排序就会有所不同。一切皆变化。