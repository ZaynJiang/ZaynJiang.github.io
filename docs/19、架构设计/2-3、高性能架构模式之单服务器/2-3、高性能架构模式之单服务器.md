## 1. 开头    
&emsp;&emsp;**高性能架构的设计决定了系统性能的上限，实现细节决定了系统性能的下限**。高性能架构设计主要集中在两方面：
* 尽量提升单服务器的性能，将单服务器的性能发挥到极致。
* 如果单服务器无法支撑性能，设计服务器集群方案  

&emsp;&emsp;做一个系统还是写一行代码，都希望能够达到高性能的效果。而达到单机的高性能是非常复杂的，因为它涉及到了：磁盘、操作系统、CPU、内存、缓存、网络、编程语言、架构多个影响因子。随随便便的一个语句就可能导致性能变差，比如debug日志等等。但是我们这里只谈探讨单服务器高性能的关键之一就是服务器采取的并发模型，因为这个可操作空间更大些  


&emsp;&emsp;并发模型有如下两个关键设计点：
* 服务器如何管理连接
* 服务器如何处理请求  

&emsp;&emsp;这两个设计点和操作系统的 I/O 模型及进程模型非常相关：
* I/O 模型：阻塞、非阻塞、同步、异步。
* 进程模型：单进程、多进程、多线程。  

下面我们一步步阐述这几种设计模式  

## 2. PPC
&emsp;&emsp;PPC 是 Process Per Connection 的缩写，其含义是指每次有新的连接就新建一个进程去专门处理这个连接的请求。  
&emsp;&emsp;PPC 模式实现简单，比较适合服务器的连接数没那么多的情况。互联网兴起后，服务器的并发和访问量从几十剧增到成千上万。这种模式基本不可行，主要原因有：
* fork 代价高，需要分配很多内核资源，需要将内存映像从父进程复制到子进程。即使现在的操作系统在复制内存映像时用到了 Copy on Write（写时复制）技术，总体来说创建进程的代价还是很大的
* 父子进程通信复杂：父进程“fork”子进程时，文件描述符可以通过内存映像复制从父进程传到子进程，但“fork”完成后，父子进程通信就比较麻烦了，需要采用 IPC（Interprocess Communication）之类的进程通信方案。例如，子进程需要在 close 之前告诉父进程自己处理了多少个请求以支撑父进程进行全局的统计，那么子进程和父进程必须采用 IPC 方案来传递信息
* 支持的并发连接数量有限：如果每个连接存活时间比较长，而且新的连接又源源不断的进来，则进程数量会越来越多，操作系统进程调度和切换的频率也越来越高，系统的压力也会越来越大。因此，一般情况下，PPC 方案能处理的并发连接数量最大也就几百

## 3. prefork  
&emsp;&emsp;由于PPC模式 fork 进程代价高，用户访问时可能感觉比较慢。  
&emsp;&emsp;顾名思义，prefork 就是提前创建进程（pre-fork）。系统在启动的时候就预先创建好进程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去 fork 进程的操作。其关键点为：  
* 多个子进程都 accept 同一个 socket
* 当有新的连接进入时，操作系统保证只有一个进程能最后 accept 成功  

该模式的缺点：
* 惊群效应，虽然只有一个子进程能 accept 成功，但所有阻塞在 accept 上的子进程都会被唤醒，这样就导致了不必要的进程调度和上下文切换了（和操作系统的版本有关系，Linux 2.6 版本后内核已经解决了 accept 惊群问题）
* 父子进程通信复杂、支持的并发连接数量有限目前实际应用也不多（Apache 服务器提供了 MPM prefork 模式，推荐在需要可靠性或者与旧软件兼容的站点时采用这种模式，默认情况下最大支持 256 个并发连接）

## 4. TPC  
&emsp;&emsp;TPC 是 Thread Per Connection 的缩写，其含义是指每次有新的连接就新建一个线程去专门处理这个连接的请求。   
&emsp;&emsp;与进程相比，线程更轻量级，创建线程的消耗比进程要少得多；同时多线程是共享进程内存空间的，线程通信相比进程通信更简单。  
&emsp;&emsp;因此，TPC 实际上是解决或者弱化了 PPC fork 代价高的问题和父子进程通信复杂的问题。  
&emsp;&emsp;和PPC相比，主进程不用“close”连接了。原因是在于子线程是共享主进程的进程空间的，连接的文件描述符并没有被复制，因此只需要一次 close 即可  

该模式的缺点：  
* 创建线程虽然比创建进程代价低，但并不是没有代价，高并发时（例如每秒上万连接）还是有性能问题。
* 无须进程间通信，但是线程间的互斥和共享又引入了复杂度，可能一不小心就导致了死锁问题。
* 多线程会出现互相影响的情况，某个线程出现异常时，可能导致整个进程退出（例如内存越界）。
* 同样存在 CPU 线程调度和切换代价的问题，TPC 方案本质上和 PPC 方案基本类似，在并发几百连接的场景下，反而更多地是采用 PPC 的方案，因为 PPC 方案不会有死锁的风险，也不会多进程互相影响，稳定性更高。

## 5. prethread
&emsp;&emsp;和 prefork 类似，prethread 模式会预先创建线程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去创建线程的操作，让用户感觉更快、体验更好   
&emsp;&emsp;prethread 的实现方式相比 prefork的优势：  
* 主进程 accept，然后将连接交给某个线程处理。
* 子线程都尝试去 accept，最终只有一个线程 accept 成功   
* prethread 理论上可以比 prefork 支持更多的并发连接，Apache 服务器 MPM worker 模式默认支持 16 × 25 = 400 个并发处理线程


**PS：Apache 服务器的 MPM worker 模式本质上就是一种 prethread 方案，但稍微做了改进。Apache 服务器会首先创建多个进程，每个进程里面再创建多个线程，这样做主要是为了考虑稳定性，即：即使某个子进程里面的某个线程异常导致整个子进程退出，还会有其他子进程继续提供服务，不会导致整个服务器全部挂掉**  
## 6. 小结
&emsp;&emsp;不同并发模式的选择，还要考察三个指标，分别是响应时间（RT），并发数（Concurrency），吞吐量（TPS）。三者关系，吞吐量=并发数/平均响应时间。不同类型的系统，对这三个指标的要求不一样:  
* 三高系统，比如秒杀、即时通信，不能使用
* 三低系统，比如ToB系统，运营类、管理类系统，一般可以使用
* 高吞吐系统，如果是内存计算为主的，一般可以使用，如果是网络IO为主的，一般不能使用。

高并发需要根据两个条件划分：连接数量，请求数量。
* 海量连接（成千上万）海量请求：例如抢购，双十一等  
  面对海量的连接至少要使用IO复用模型或者异步IO模型，针对海量的请求，无论使用多进程处理还是多线程，单机都是无法支撑的，应该集群了,单机也要支持很多连接，不然集群成本太高
* 常量连接（几十上百）海量请求：例如中间件  
  常量连接，如使用TPC的preyhtead模型，启动几十上百的线程去处理连接，应该问题不大吧，但是老师举的列子是中间件是这类系统，我就有点疑问了，是不是中间件系统都可以是阻塞IO模型来实现，比如activemq既支持BIO也支持NIO，但是NIO只是解决了能处理更多的连接，而真正每个请求的处理快慢还得看后面的业务的处理；而阿里的rocketmq也是使用netty这样的NIO框架实现的。但在面对常量连接的场景下，NIO并没有优势啊。
* 海量连接常量请求：例如门户网站  
  这种系统我觉得非常适合使用netty这样的NIO框架来实现，IO复用模型可以处理海量的连接，而每个连接的请求数据量会很小，处理会很长快，如华仔说的门户网站，只要简单返回页面即可。
* 常量连接常量请求：例如内部运营系统，管理系统  
  这种系统，本讲的模式就很适合了。  

**注意：PPC和TPC对那些吞吐量比较大，长连接且连接数不多的系统应该比较适用。两种模式的特点都比较重，每个连接都能占有较多计算资源，一些内部系统，如日志系统用于实时监控的估计可以采用。这类型的系统一般连接数不多，吞吐量比较大，不求服务数量，求服务质量**
## 7. Reactor

## 8. Proactor  


## 9. 总结