## 1. 存储方式

Prometheus提供了两种存储方式：

* 本地存储
* 远程存储

### 1.1. 抽象接口

为了适配各种远端存储，Prometheus抽象了一组数据读写接口，并通过适配器将Prometheus对远端存储的读写接口转化为第三方的数据存储接口，从而扩展了Prometheus存储大量历史数据的能力。

#### 2.1. 添加数据

appender接口：add、addFirst、commit、rollback

#### 2.2. 查询数据



## 2. 本地存储

本地存储也会实现apender接口，为了兼容本地和远程，提供了fanount接口实现了appender接口，会本地远程一起存储

### 2.1. leveldb

最开始的时序数据库。

数据和元数据都被保存在 LeveIDB中，并且每隔15min刷新一次，这就产生了一个严重的问题。如果 Prometheus宕机，则可能会丢失 15min的监控数据。性能不高，受限于LeveIDB的存储性能，每秒只能接收50 000个样本，这就意味着:如果按照每台机器1000个指标、采集周期10s计算,单台Prometheus最多采集500台机器的数据。

### 2.2. tsdb

tsdb有两大核心概念:

* block

  包含chunk、index、meta.json、tombstones

* wal

#### 2.2.1. block

block的划分是根据时间维度进行的。按照步数递增划分，比如步数为3，2h、6h、18h。

唯一名称ulid，为16字节，前6字节为时间戳，后10字节为随机数。

一个blcok由4个组件组成

* chunk

  固定512m大小，多个按照序号编号

* index

  索引，记录数据在chunk中的偏移量

* meta.json

  记录了block元信息，起始时间，结束时间，数据源、样本数、chunk数、压缩次数

* tombstones

  用于删除行数据后的软删除记录.

第一个block存储在内存中，允许修改，其它的block只读的形式存储在硬盘之中。block初始设定为2h数据

#### 2.2.2. wal

预写日志，用于是实现事务的，操作前将数据记录下来，以便后面回滚。

这个类似与mysql的redo日志，Prometheus会将周期性采集的监控数据通过Add接口添加到head block中，但这些数据没有被持久化，TSDB通过WAL将提交的数据先保存到磁盘中，在TSDB宕机重启后，会首先启动协程读取WAL从而进行恢复.

#### 2.2.3. 存储时效

默认15d，但是无论是调整为保存4h还是保存5h，都最多可以查询11h的监控数据。这是因为Prometheus 在计算时排除了head block 和最新生成block，是从 t-5h开始计算的。

#### 2.2.4. 存储路径

存储在本地磁盘之中，wal也会放在这里

## 3. 远程存储

### 3.1. 远程配置

配置adapter地址，超时时间、请求队列等

### 3.2. 存储架构

Prometheus会将数据先打到adapter上

每种Adapter 都通过实现读接口和写接口对接到Prometheus，

Adapter也可以只实现其中一个接口。若通过Adapter实现OpenTSDB的一个写接口,那么Prometheus会将数据写入OpenTSDB中，对监控数据的查询可以借助OpenTSDB原生的查询接口实现。



### 3.3. 数据聚合

在配置远端存储后，Prometheus的本地存储仍然有效

查找监控数据时， Prometheus在对远端存储查询之前先用本地存储中最老数据的时间戳与查询最老数据的时间戳进行比较，若发现需要查询的数据在本地存储中，则会跳过对远端存储的查询;如果不全在本地存储中，则将远端存储查询的最大时间修改成本地存储的最大时间，避免重复查询。比如如果查询15d内的监控数据，Prometheus就会直接从本地读取，如果超过15d，则会将远端存储中 15d外的数据和本地存储中 15d内的数据合并后返回。

## 4. 小结